{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, regularizers, callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a dict which consists label for each image path and its particular data.\n",
    "def load_file(file_path, label):\n",
    "\n",
    "    # declare the folder name\n",
    "    folder_name = file_path.split(\"/\")[-1]\n",
    "    # declare output list\n",
    "    out_list = []\n",
    "    # load every file that .png format\n",
    "    for image_path in glob.glob(file_path + \"/*.*\"):\n",
    "        # read image file\n",
    "        # image = imageio.imread(image_path)\n",
    "        image = imageio.v2.imread(image_path)   \n",
    "        # print(image_path)\n",
    "        # declare temporary dict dtype\n",
    "        temp = {}\n",
    "        # set the file name\n",
    "        temp[\"name\"] = image_path.split(\"/\")[-1]\n",
    "        # set the file label, 0 for non defect. 1 for defect\n",
    "        temp[\"label\"] = label\n",
    "\n",
    "        # There are somes images are tensor dtype\n",
    "        # Thus I fix by selecting only a tensor index zero\n",
    "        try:   \n",
    "            temp[\"data\"] = image[:,:,0].astype(\"int\") \n",
    "        except:\n",
    "            # normal case\n",
    "            temp[\"data\"] = image.astype(\"int\")\n",
    "        # append temp into output list\n",
    "        out_list.append(temp)\n",
    "    # print process status by checking size of output list\n",
    "    if len(out_list) == 0:\n",
    "        print(\"loading files from folder: {} is failed\".format(folder_name))\n",
    "    else:\n",
    "        print(\"loading file from folder: {} is successful\".format(folder_name))\n",
    "    # convert list into numpy array dtype\n",
    "    return np.array(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "acnitic_images_path      =  r\"C:/Users/heman/Downloads/Capstone_Project/SKIN/augmented_new_data/A/\"\n",
    "derma_images_path =  r\"C:/Users/heman/Downloads/Capstone_Project/SKIN/augmented_new_data/D/\"\n",
    "vascular_images_path =  r\"C:/Users/heman/Downloads/Capstone_Project/SKIN/augmented_new_data/V/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file from folder:  is successful\n",
      "loading file from folder:  is successful\n",
      "loading file from folder:  is successful\n"
     ]
    }
   ],
   "source": [
    "acnitic_images_path = load_file(file_path=acnitic_images_path, label=0)\n",
    "derma_images_path = load_file(file_path=derma_images_path, label=1)\n",
    "vascular_images_path = load_file(file_path=vascular_images_path, label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1180,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acnitic_images_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derma_images_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1307,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vascular_images_path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare and clean the data to avoid error during model fitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Size: 1161\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(acnitic_images_path)\n",
    "np.random.shuffle(derma_images_path)\n",
    "np.random.shuffle(vascular_images_path)\n",
    "\n",
    "# the class size is the min length compared with defect-free and defect images, we do this in orde to balance the dataset.\n",
    "if (acnitic_images_path.shape[0] <= derma_images_path.shape[0]) and (acnitic_images_path.shape[0] <= vascular_images_path.shape[0]):\n",
    "  class_size = acnitic_images_path.shape[0]\n",
    "elif (derma_images_path.shape[0] <= vascular_images_path.shape[0]) and (derma_images_path.shape[0] <= acnitic_images_path.shape[0]):\n",
    "  class_size = derma_images_path.shape[0]\n",
    "else:\n",
    "  class_size = vascular_images_path.shape[0]\n",
    "print(\"Class Size:\", class_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3483, 180, 180, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([1161, 1161, 1161], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we shuffle the order of defect-free and defect images\n",
    "np.random.shuffle(acnitic_images_path)\n",
    "np.random.shuffle(derma_images_path)\n",
    "np.random.shuffle(vascular_images_path)\n",
    "\n",
    "# Concatenate both the datasets with size as class_size.\n",
    "dataset = np.concatenate((acnitic_images_path[:class_size], derma_images_path[:class_size], vascular_images_path[:class_size]))\n",
    "\n",
    "# create an empty matrix X of 256x4096 and has dataset length row, which holds all the data i.e images from dataset.\n",
    "# Independent Features -> X\n",
    "X = np.empty([dataset.shape[0], 180, 180]).astype(int)\n",
    "\n",
    "# create vector y which has dataset length, which holds all the labels for our data, this is jsut similar to partitioning the data before splitting, \n",
    "# Target_variable -> y\n",
    "y = np.empty(dataset.shape[0]).astype(int)\n",
    "\n",
    "# assign the X,y one-by-one\n",
    "for i in range(dataset.shape[0]):\n",
    "    X[i] = dataset[i][\"data\"]\n",
    "    y[i] = dataset[i][\"label\"]\n",
    "\n",
    "# since Keras acquire the Image input in a tensor type -> we reshape X\n",
    "X = X.reshape(X.shape[0], 180, 180, 1)\n",
    "print(X.shape)\n",
    "\n",
    "# display size of the label 0 and label 1 \n",
    "np.unique(y, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 180, 180, 1)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 180, 180, 16)      160       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 180, 180, 16)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 90, 90, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 90, 90, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 45, 45, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 45, 45, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 22, 22, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 22, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 30976)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               3965056   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,988,739\n",
      "Trainable params: 3,988,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1e06286b4c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_model(image_shape = (180, 180, 1), print_summary = True):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.experimental.preprocessing.Rescaling(1./255, input_shape=(180, 180, 1)))\n",
    "    model.add(layers.Conv2D(16, 3, padding='same'))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "\n",
    "    model.add(layers.Conv2D(32, 3, padding='same'))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "\n",
    "    model.add(layers.Conv2D(64, 3, padding='same'))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.20))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dense(3, activation = 'sigmoid'))\n",
    "\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    \n",
    "    # show the CNN model detail\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "def train_model(model, xtrain, ytrain, xval, yval, n_epoch, batch_size):\n",
    "    # train CNN model\n",
    "    # batch size to reduce memory usage\n",
    "    # set early stopping to avoid overfitting\n",
    "    \n",
    "    earlystopping = EarlyStopping(monitor='accuracy', patience=2)\n",
    "    filepath = \"C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy/weights-best-{epoch:02d}-{accuracy:.2f}-{val_accuracy:.2f}.hdf5\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint, earlystopping]\n",
    "\n",
    "    history = model.fit(xtrain, ytrain, epochs=n_epoch, batch_size=batch_size, validation_data=(xval, yval), callbacks=[callbacks_list])\n",
    "    return history\n",
    "\n",
    "\n",
    "create_model(image_shape=(180, 180, 1), print_summary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Export CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: number of samples each class: (array([0, 1, 2]), array([1053, 1047, 1034], dtype=int64))\n",
      "y_test: number of samples each class: (array([0, 1, 2]), array([108, 114, 127], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=23)\n",
    "print(\"y_train: number of samples each class: {}\".format(np.unique(y_train, return_counts=True)))\n",
    "print(\"y_test: number of samples each class: {}\".format(np.unique(y_test, return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 180, 180, 1)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 180, 180, 16)      160       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 180, 180, 16)      0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 90, 90, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 90, 90, 32)        4640      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 90, 90, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 45, 45, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 45, 45, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 22, 22, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 22, 22, 64)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 30976)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               3965056   \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,988,739\n",
      "Trainable params: 3,988,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = None\n",
    "cnn_model = create_model(image_shape=(180, 180, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='accuracy', patience=5)\n",
    "filepath = \"C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy/weights-best-{epoch:02d}-{accuracy:.2f}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heman\\anaconda3\\lib\\site-packages\\keras\\backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - ETA: 0s - loss: 1.0788 - accuracy: 0.3674\n",
      "Epoch 1: accuracy improved from -inf to 0.36738, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-01-0.37-0.38.hdf5\n",
      "282/282 [==============================] - 70s 236ms/step - loss: 1.0788 - accuracy: 0.3674 - val_loss: 1.0631 - val_accuracy: 0.3790\n",
      "Epoch 2/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 1.0267 - accuracy: 0.4135\n",
      "Epoch 2: accuracy improved from 0.36738 to 0.41348, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-02-0.41-0.41.hdf5\n",
      "282/282 [==============================] - 66s 235ms/step - loss: 1.0267 - accuracy: 0.4135 - val_loss: 1.0369 - val_accuracy: 0.4076\n",
      "Epoch 3/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.9963 - accuracy: 0.4401\n",
      "Epoch 3: accuracy improved from 0.41348 to 0.44007, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-03-0.44-0.43.hdf5\n",
      "282/282 [==============================] - 66s 234ms/step - loss: 0.9963 - accuracy: 0.4401 - val_loss: 1.0072 - val_accuracy: 0.4299\n",
      "Epoch 4/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.9770 - accuracy: 0.4582\n",
      "Epoch 4: accuracy improved from 0.44007 to 0.45816, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-04-0.46-0.42.hdf5\n",
      "282/282 [==============================] - 67s 237ms/step - loss: 0.9770 - accuracy: 0.4582 - val_loss: 1.0219 - val_accuracy: 0.4236\n",
      "Epoch 5/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.9647 - accuracy: 0.4713\n",
      "Epoch 5: accuracy improved from 0.45816 to 0.47128, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-05-0.47-0.47.hdf5\n",
      "282/282 [==============================] - 67s 237ms/step - loss: 0.9647 - accuracy: 0.4713 - val_loss: 1.0840 - val_accuracy: 0.4713\n",
      "Epoch 6/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.9297 - accuracy: 0.5181\n",
      "Epoch 6: accuracy improved from 0.47128 to 0.51809, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-06-0.52-0.47.hdf5\n",
      "282/282 [==============================] - 67s 236ms/step - loss: 0.9297 - accuracy: 0.5181 - val_loss: 1.0127 - val_accuracy: 0.4682\n",
      "Epoch 7/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.8866 - accuracy: 0.5592\n",
      "Epoch 7: accuracy improved from 0.51809 to 0.55922, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-07-0.56-0.54.hdf5\n",
      "282/282 [==============================] - 66s 235ms/step - loss: 0.8866 - accuracy: 0.5592 - val_loss: 0.9408 - val_accuracy: 0.5382\n",
      "Epoch 8/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.8765 - accuracy: 0.5564\n",
      "Epoch 8: accuracy did not improve from 0.55922\n",
      "282/282 [==============================] - 68s 243ms/step - loss: 0.8765 - accuracy: 0.5564 - val_loss: 1.0449 - val_accuracy: 0.5414\n",
      "Epoch 9/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.8170 - accuracy: 0.6046\n",
      "Epoch 9: accuracy improved from 0.55922 to 0.60461, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-09-0.60-0.51.hdf5\n",
      "282/282 [==============================] - 67s 238ms/step - loss: 0.8170 - accuracy: 0.6046 - val_loss: 0.9905 - val_accuracy: 0.5127\n",
      "Epoch 10/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.7815 - accuracy: 0.6128\n",
      "Epoch 10: accuracy improved from 0.60461 to 0.61277, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-10-0.61-0.54.hdf5\n",
      "282/282 [==============================] - 75s 266ms/step - loss: 0.7815 - accuracy: 0.6128 - val_loss: 1.0543 - val_accuracy: 0.5446\n",
      "Epoch 11/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.7498 - accuracy: 0.6454\n",
      "Epoch 11: accuracy improved from 0.61277 to 0.64539, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-11-0.65-0.55.hdf5\n",
      "282/282 [==============================] - 65s 229ms/step - loss: 0.7498 - accuracy: 0.6454 - val_loss: 1.0832 - val_accuracy: 0.5478\n",
      "Epoch 12/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.6798\n",
      "Epoch 12: accuracy improved from 0.64539 to 0.67979, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-12-0.68-0.58.hdf5\n",
      "282/282 [==============================] - 65s 230ms/step - loss: 0.7008 - accuracy: 0.6798 - val_loss: 1.0594 - val_accuracy: 0.5828\n",
      "Epoch 13/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.6205 - accuracy: 0.7113\n",
      "Epoch 13: accuracy improved from 0.67979 to 0.71135, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-13-0.71-0.60.hdf5\n",
      "282/282 [==============================] - 66s 233ms/step - loss: 0.6205 - accuracy: 0.7113 - val_loss: 1.1885 - val_accuracy: 0.6019\n",
      "Epoch 14/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.5362 - accuracy: 0.7589\n",
      "Epoch 14: accuracy improved from 0.71135 to 0.75887, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-14-0.76-0.64.hdf5\n",
      "282/282 [==============================] - 66s 234ms/step - loss: 0.5362 - accuracy: 0.7589 - val_loss: 1.1270 - val_accuracy: 0.6369\n",
      "Epoch 15/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4648 - accuracy: 0.7897\n",
      "Epoch 15: accuracy improved from 0.75887 to 0.78972, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-15-0.79-0.66.hdf5\n",
      "282/282 [==============================] - 66s 235ms/step - loss: 0.4648 - accuracy: 0.7897 - val_loss: 1.3943 - val_accuracy: 0.6561\n",
      "Epoch 16/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4138 - accuracy: 0.8270\n",
      "Epoch 16: accuracy improved from 0.78972 to 0.82695, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-16-0.83-0.63.hdf5\n",
      "282/282 [==============================] - 66s 233ms/step - loss: 0.4138 - accuracy: 0.8270 - val_loss: 1.3945 - val_accuracy: 0.6274\n",
      "Epoch 17/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3427 - accuracy: 0.8660\n",
      "Epoch 17: accuracy improved from 0.82695 to 0.86596, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-17-0.87-0.67.hdf5\n",
      "282/282 [==============================] - 67s 236ms/step - loss: 0.3427 - accuracy: 0.8660 - val_loss: 1.3055 - val_accuracy: 0.6656\n",
      "Epoch 18/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.3265 - accuracy: 0.8812\n",
      "Epoch 18: accuracy improved from 0.86596 to 0.88121, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-18-0.88-0.61.hdf5\n",
      "282/282 [==============================] - 66s 232ms/step - loss: 0.3265 - accuracy: 0.8812 - val_loss: 1.4292 - val_accuracy: 0.6146\n",
      "Epoch 19/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2366 - accuracy: 0.9184\n",
      "Epoch 19: accuracy improved from 0.88121 to 0.91844, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-19-0.92-0.66.hdf5\n",
      "282/282 [==============================] - 67s 237ms/step - loss: 0.2366 - accuracy: 0.9184 - val_loss: 1.4243 - val_accuracy: 0.6592\n",
      "Epoch 20/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9397\n",
      "Epoch 20: accuracy improved from 0.91844 to 0.93972, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-20-0.94-0.67.hdf5\n",
      "282/282 [==============================] - 66s 234ms/step - loss: 0.1811 - accuracy: 0.9397 - val_loss: 1.8627 - val_accuracy: 0.6656\n",
      "Epoch 21/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9514\n",
      "Epoch 21: accuracy improved from 0.93972 to 0.95142, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-21-0.95-0.71.hdf5\n",
      "282/282 [==============================] - 64s 228ms/step - loss: 0.1396 - accuracy: 0.9514 - val_loss: 1.8977 - val_accuracy: 0.7070\n",
      "Epoch 22/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9617\n",
      "Epoch 22: accuracy improved from 0.95142 to 0.96170, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-22-0.96-0.69.hdf5\n",
      "282/282 [==============================] - 67s 236ms/step - loss: 0.1135 - accuracy: 0.9617 - val_loss: 2.2303 - val_accuracy: 0.6943\n",
      "Epoch 23/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9585\n",
      "Epoch 23: accuracy did not improve from 0.96170\n",
      "282/282 [==============================] - 65s 231ms/step - loss: 0.1399 - accuracy: 0.9585 - val_loss: 1.8042 - val_accuracy: 0.7006\n",
      "Epoch 24/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9628\n",
      "Epoch 24: accuracy improved from 0.96170 to 0.96277, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-24-0.96-0.67.hdf5\n",
      "282/282 [==============================] - 64s 228ms/step - loss: 0.1116 - accuracy: 0.9628 - val_loss: 2.5827 - val_accuracy: 0.6688\n",
      "Epoch 25/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9837\n",
      "Epoch 25: accuracy improved from 0.96277 to 0.98369, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-25-0.98-0.68.hdf5\n",
      "282/282 [==============================] - 65s 230ms/step - loss: 0.0568 - accuracy: 0.9837 - val_loss: 2.6601 - val_accuracy: 0.6752\n",
      "Epoch 26/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9894\n",
      "Epoch 26: accuracy improved from 0.98369 to 0.98936, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-26-0.99-0.69.hdf5\n",
      "282/282 [==============================] - 65s 229ms/step - loss: 0.0441 - accuracy: 0.9894 - val_loss: 2.6806 - val_accuracy: 0.6943\n",
      "Epoch 27/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9933\n",
      "Epoch 27: accuracy improved from 0.98936 to 0.99326, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-27-0.99-0.70.hdf5\n",
      "282/282 [==============================] - 65s 230ms/step - loss: 0.0302 - accuracy: 0.9933 - val_loss: 2.6203 - val_accuracy: 0.7006\n",
      "Epoch 28/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9901\n",
      "Epoch 28: accuracy did not improve from 0.99326\n",
      "282/282 [==============================] - 65s 229ms/step - loss: 0.0308 - accuracy: 0.9901 - val_loss: 2.8927 - val_accuracy: 0.6879\n",
      "Epoch 29/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9663\n",
      "Epoch 29: accuracy did not improve from 0.99326\n",
      "282/282 [==============================] - 65s 230ms/step - loss: 0.1214 - accuracy: 0.9663 - val_loss: 2.5272 - val_accuracy: 0.6783\n",
      "Epoch 30/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9904\n",
      "Epoch 30: accuracy did not improve from 0.99326\n",
      "282/282 [==============================] - 63s 224ms/step - loss: 0.0414 - accuracy: 0.9904 - val_loss: 2.1491 - val_accuracy: 0.6943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e04a518e80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to run this, if you are loading the model, if not you cna run this to train your model again and save another weights file.\n",
    "cnn_model.fit(X_train, y_train, batch_size=10, epochs=20, validation_split=0.1, callbacks=callbacks_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.25661563873291, 0.6590257883071899)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, acc = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the training where we have stopped.\n",
    "# cnn_model.fit(X_train, y_train, batch_size=10, initial_epochs=10, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('C:/Users/heman/Downloads/Capstone_Project/SKIN/models/weights-best-04-0.91-0.73.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4080801010131836, 0.8767908215522766)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, acc = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.8433\n",
      "Epoch 1: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 64s 224ms/step - loss: 0.4727 - accuracy: 0.8433 - val_loss: 0.5704 - val_accuracy: 0.7771\n",
      "Epoch 2/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.9004\n",
      "Epoch 2: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 62s 221ms/step - loss: 0.2866 - accuracy: 0.9004 - val_loss: 0.3835 - val_accuracy: 0.8758\n",
      "Epoch 3/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9245\n",
      "Epoch 3: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 62s 221ms/step - loss: 0.2075 - accuracy: 0.9245 - val_loss: 0.4105 - val_accuracy: 0.8790\n",
      "Epoch 4/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1750 - accuracy: 0.9397\n",
      "Epoch 4: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 62s 221ms/step - loss: 0.1750 - accuracy: 0.9397 - val_loss: 0.5189 - val_accuracy: 0.7962\n",
      "Epoch 5/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9507\n",
      "Epoch 5: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 63s 222ms/step - loss: 0.1417 - accuracy: 0.9507 - val_loss: 0.4641 - val_accuracy: 0.8631\n",
      "Epoch 6/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9656\n",
      "Epoch 6: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 64s 226ms/step - loss: 0.1044 - accuracy: 0.9656 - val_loss: 0.6297 - val_accuracy: 0.8662\n",
      "Epoch 7/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9663\n",
      "Epoch 7: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 63s 224ms/step - loss: 0.1110 - accuracy: 0.9663 - val_loss: 0.7220 - val_accuracy: 0.7930\n",
      "Epoch 8/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9787\n",
      "Epoch 8: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 67s 238ms/step - loss: 0.0751 - accuracy: 0.9787 - val_loss: 0.7399 - val_accuracy: 0.8025\n",
      "Epoch 9/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9755\n",
      "Epoch 9: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 64s 226ms/step - loss: 0.0761 - accuracy: 0.9755 - val_loss: 0.8177 - val_accuracy: 0.8217\n",
      "Epoch 10/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9816\n",
      "Epoch 10: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 63s 224ms/step - loss: 0.0602 - accuracy: 0.9816 - val_loss: 0.7102 - val_accuracy: 0.8344\n",
      "Epoch 11/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9819\n",
      "Epoch 11: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 73s 258ms/step - loss: 0.0622 - accuracy: 0.9819 - val_loss: 0.7500 - val_accuracy: 0.8535\n",
      "Epoch 12/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9869\n",
      "Epoch 12: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 66s 236ms/step - loss: 0.0496 - accuracy: 0.9869 - val_loss: 0.7924 - val_accuracy: 0.8089\n",
      "Epoch 13/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9915\n",
      "Epoch 13: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 68s 242ms/step - loss: 0.0348 - accuracy: 0.9915 - val_loss: 0.7592 - val_accuracy: 0.8535\n",
      "Epoch 14/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9723\n",
      "Epoch 14: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 69s 243ms/step - loss: 0.0826 - accuracy: 0.9723 - val_loss: 0.8445 - val_accuracy: 0.8057\n",
      "Epoch 15/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9894\n",
      "Epoch 15: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 68s 240ms/step - loss: 0.0388 - accuracy: 0.9894 - val_loss: 0.8029 - val_accuracy: 0.8312\n",
      "Epoch 16/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9940\n",
      "Epoch 16: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 68s 241ms/step - loss: 0.0199 - accuracy: 0.9940 - val_loss: 0.7680 - val_accuracy: 0.8503\n",
      "Epoch 17/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9950\n",
      "Epoch 17: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 68s 240ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.7459 - val_accuracy: 0.8376\n",
      "Epoch 18/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9943\n",
      "Epoch 18: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 68s 242ms/step - loss: 0.0234 - accuracy: 0.9943 - val_loss: 0.8337 - val_accuracy: 0.8185\n",
      "Epoch 19/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9617\n",
      "Epoch 19: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 68s 241ms/step - loss: 0.1458 - accuracy: 0.9617 - val_loss: 0.7051 - val_accuracy: 0.8280\n",
      "Epoch 20/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9915\n",
      "Epoch 20: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 70s 249ms/step - loss: 0.0348 - accuracy: 0.9915 - val_loss: 1.0592 - val_accuracy: 0.7771\n",
      "Epoch 21/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9957\n",
      "Epoch 21: accuracy did not improve from 0.99787\n",
      "282/282 [==============================] - 69s 245ms/step - loss: 0.0154 - accuracy: 0.9957 - val_loss: 0.8245 - val_accuracy: 0.8057\n",
      "Epoch 22/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9993\n",
      "Epoch 22: accuracy improved from 0.99787 to 0.99929, saving model to C:/Users/heman/Downloads/Capstone_Project/SKIN/models/different_class_accuracy\\weights-best-22-1.00-0.82.hdf5\n",
      "282/282 [==============================] - 67s 236ms/step - loss: 0.0063 - accuracy: 0.9993 - val_loss: 0.8113 - val_accuracy: 0.8248\n",
      "Epoch 23/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0052 - accuracy: 0.9989\n",
      "Epoch 23: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 68s 241ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.9911 - val_accuracy: 0.8248\n",
      "Epoch 24/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9940\n",
      "Epoch 24: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 67s 236ms/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 1.1850 - val_accuracy: 0.7898\n",
      "Epoch 25/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9759\n",
      "Epoch 25: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 67s 236ms/step - loss: 0.0879 - accuracy: 0.9759 - val_loss: 0.9228 - val_accuracy: 0.8121\n",
      "Epoch 26/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9926\n",
      "Epoch 26: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 61s 215ms/step - loss: 0.0375 - accuracy: 0.9926 - val_loss: 1.3354 - val_accuracy: 0.7484\n",
      "Epoch 27/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9879\n",
      "Epoch 27: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 52s 185ms/step - loss: 0.0521 - accuracy: 0.9879 - val_loss: 0.8434 - val_accuracy: 0.8439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0a9f97fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to run this, if you are loading the model, if not you cna run this to train your model again and save another weights file.\n",
    "loaded_model.fit(X_train, y_train, batch_size=10, epochs=30, validation_split=0.1, callbacks=callbacks_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8856669664382935, 0.8051576018333435)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, acc = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model_2 = load_model('C:/Users/heman/Downloads/Capstone_Project/SKIN/models/weights-best-04-0.91-0.73.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4080801010131836, 0.8767908215522766)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, acc = loaded_model_2.evaluate(X_test, y_test, verbose=0)\n",
    "score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.4491 - accuracy: 0.8489\n",
      "Epoch 1: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 53s 187ms/step - loss: 0.4491 - accuracy: 0.8489 - val_loss: 0.3927 - val_accuracy: 0.8599\n",
      "Epoch 2/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 0.9067\n",
      "Epoch 2: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 61s 216ms/step - loss: 0.2740 - accuracy: 0.9067 - val_loss: 0.3942 - val_accuracy: 0.8694\n",
      "Epoch 3/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.9298\n",
      "Epoch 3: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 51s 180ms/step - loss: 0.2025 - accuracy: 0.9298 - val_loss: 0.4342 - val_accuracy: 0.8662\n",
      "Epoch 4/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 0.9450\n",
      "Epoch 4: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 49s 172ms/step - loss: 0.1652 - accuracy: 0.9450 - val_loss: 0.4501 - val_accuracy: 0.8917\n",
      "Epoch 5/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9621\n",
      "Epoch 5: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 49s 174ms/step - loss: 0.1132 - accuracy: 0.9621 - val_loss: 0.5848 - val_accuracy: 0.8471\n",
      "Epoch 6/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9667\n",
      "Epoch 6: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 54s 191ms/step - loss: 0.1014 - accuracy: 0.9667 - val_loss: 0.5242 - val_accuracy: 0.8599\n",
      "Epoch 7/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9652\n",
      "Epoch 7: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 54s 192ms/step - loss: 0.0972 - accuracy: 0.9652 - val_loss: 0.5971 - val_accuracy: 0.8503\n",
      "Epoch 8/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9787\n",
      "Epoch 8: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 52s 183ms/step - loss: 0.0733 - accuracy: 0.9787 - val_loss: 0.5938 - val_accuracy: 0.8567\n",
      "Epoch 9/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9883\n",
      "Epoch 9: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 48s 170ms/step - loss: 0.0446 - accuracy: 0.9883 - val_loss: 0.8409 - val_accuracy: 0.8439\n",
      "Epoch 10/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9748\n",
      "Epoch 10: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 47s 168ms/step - loss: 0.0780 - accuracy: 0.9748 - val_loss: 0.5616 - val_accuracy: 0.8503\n",
      "Epoch 11/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9837\n",
      "Epoch 11: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 56s 198ms/step - loss: 0.0583 - accuracy: 0.9837 - val_loss: 0.6772 - val_accuracy: 0.8535\n",
      "Epoch 12/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9883\n",
      "Epoch 12: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 68s 241ms/step - loss: 0.0421 - accuracy: 0.9883 - val_loss: 0.8913 - val_accuracy: 0.8185\n",
      "Epoch 13/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9670\n",
      "Epoch 13: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 70s 247ms/step - loss: 0.1183 - accuracy: 0.9670 - val_loss: 0.9546 - val_accuracy: 0.8121\n",
      "Epoch 14/30\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9716\n",
      "Epoch 14: accuracy did not improve from 0.99929\n",
      "282/282 [==============================] - 61s 216ms/step - loss: 0.1042 - accuracy: 0.9716 - val_loss: 0.7524 - val_accuracy: 0.8089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0aa07ddf0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to run this, if you are loading the model, if not you cna run this to train your model again and save another weights file.\n",
    "loaded_model_2.fit(X_train, y_train, batch_size=10, epochs=30, validation_split=0.1, callbacks=callbacks_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model_3 = load_model('C:/Users/heman/Downloads/Capstone_Project/SKIN/models/weights-best-04-0.91-0.73.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4080801010131836, 0.8767908215522766)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score, acc = loaded_model_3.evaluate(X_test, y_test, verbose=0)\n",
    "score, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 76ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_model_3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.8456867e-01, 7.5186568e-01, 1.6591695e-01],\n",
       "       [1.9171443e-08, 9.9924856e-01, 9.7347993e-01],\n",
       "       [9.2867756e-01, 2.7004763e-01, 7.0721649e-02],\n",
       "       ...,\n",
       "       [9.6644032e-01, 4.6402359e-01, 2.3293778e-02],\n",
       "       [7.8876156e-01, 8.6066008e-01, 2.0716643e-01],\n",
       "       [2.6251048e-02, 6.4301395e-01, 9.5236158e-01]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(y_pred)\n",
    "result = []\n",
    "for i in range(count):\n",
    "    if np.argmax(y_pred[i]) == 0:\n",
    "        result.append(0)\n",
    "    elif np.argmax(y_pred[i]) == 1:\n",
    "        result.append(1)\n",
    "    else:\n",
    "        result.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6AAAANVCAYAAABmmpMbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQEklEQVR4nO3debhVBb0//vdmOgwChsSkqOCsmCImV9TAVMyZn5mWlvMIlqipETlWoFyvUg44i5Vm3puWmprkgBpaalrmgJk4y8URlBnO/v3h13M7wVaGzdpwfL2eZz951lp7788+z3Oqt5/P+uxSuVwuBwAAAFawZrUuAAAAgM8GARQAAIBCCKAAAAAUQgAFAACgEAIoAAAAhRBAAQAAKIQACgAAQCEEUAAAAAohgAIAAFAIARRgJfO3v/0thx12WHr16pXWrVtntdVWy1ZbbZUxY8bk3XffXaHv/cQTT2TgwIHp2LFjSqVSxo4dW/X3KJVKOeuss6r+up9m/PjxKZVKKZVKuf/++xc5Xy6Xs/7666dUKmXQoEHL9B6XXnppxo8fv1TPuf/++yvWBABNTYtaFwDA/7nyyiszdOjQbLTRRjnllFOy6aabZv78+Xnsscdy2WWX5eGHH84tt9yywt7/8MMPz8yZM3PjjTfmc5/7XNZdd92qv8fDDz+ctdZaq+qvu6Tat2+fq6++epGQOXHixPzzn/9M+/btl/m1L7300nTu3DmHHnroEj9nq622ysMPP5xNN910md8XAFYVAijASuLhhx/Occcdl1122SW/+c1vUldX13Bul112ycknn5y77rprhdbw97//PUcddVR22223FfYe//Ef/7HCXntJHHDAAbn++utzySWXpEOHDg3Hr7766my77baZMWNGIXXMnz8/pVIpHTp0qPnvBACKYgQXYCUxatSolEqlXHHFFY3C58datWqVvffeu+Hn+vr6jBkzJhtvvHHq6urSpUuXHHzwwXnttdcaPW/QoEHp06dPHn300eywww5p27ZtevfunXPPPTf19fVJ/m88dcGCBRk3blzDqGqSnHXWWQ3//K8+fs5LL73UcOzee+/NoEGDssYaa6RNmzZZe+2189WvfjWzZs1quGZxI7h///vfs88+++Rzn/tcWrdunS233DLXXXddo2s+HlX95S9/mZEjR6ZHjx7p0KFDdt5550yePHnJfslJvvGNbyRJfvnLXzYcmz59en7961/n8MMPX+xzzj777PTv3z+dOnVKhw4dstVWW+Xqq69OuVxuuGbdddfN008/nYkTJzb8/j7uIH9c+89//vOcfPLJWXPNNVNXV5cXXnhhkRHct99+Oz179syAAQMyf/78htd/5pln0q5du3zrW99a4s8KACsbARRgJbBw4cLce++96devX3r27LlEzznuuONy2mmnZZdddsmtt96aH/7wh7nrrrsyYMCAvP32242unTp1ag466KB885vfzK233prddtstI0aMyC9+8YskyR577JGHH344SbLffvvl4Ycfbvh5Sb300kvZY4890qpVq1xzzTW56667cu6556Zdu3aZN29exedNnjw5AwYMyNNPP52f/vSnufnmm7Ppppvm0EMPzZgxYxa5/vvf/35efvnlXHXVVbniiivyj3/8I3vttVcWLly4RHV26NAh++23X6655pqGY7/85S/TrFmzHHDAARU/2zHHHJObbropN998c/bdd998+9vfzg9/+MOGa2655Zb07t07ffv2bfj9/fu49IgRI/LKK6/ksssuy2233ZYuXbos8l6dO3fOjTfemEcffTSnnXZakmTWrFn52te+lrXXXjuXXXbZEn1OAFgZGcEFWAm8/fbbmTVrVnr16rVE1z/33HO54oorMnTo0Fx00UUNx/v27Zv+/fvnwgsvzI9//OOG4++8807uuOOObLPNNkmSnXfeOffff39uuOGGHHzwwfn85z+fz3/+80mSrl27LtNI6OOPP545c+bkP//zP7PFFls0HD/wwAM/8XlnnXVW5s2bl/vuu68hfO++++55//33c/bZZ+eYY45Jx44dG67fdNNNG4JzkjRv3jz7779/Hn300SWu+/DDD8+OO+6Yp59+OptttlmuueaafO1rX6t4/+e1117b8M/19fUZNGhQyuVyfvKTn+T0009PqVRK375906ZNm08cqV1vvfXy3//9359a33bbbZcf//jHOe200/KlL30pv/nNbzJlypT86U9/Srt27ZboMwLAykgHFGAVdN999yXJIstuttlmm2yyySa55557Gh3v1q1bQ/j82Be+8IW8/PLLVatpyy23TKtWrXL00Ufnuuuuy4svvrhEz7v33nuz0047LdL5PfTQQzNr1qxFOrH/OoacfPQ5kizVZxk4cGDWW2+9XHPNNXnqqafy6KOPVhy//bjGnXfeOR07dkzz5s3TsmXLnHHGGXnnnXcybdq0JX7fr371q0t87SmnnJI99tgj3/jGN3Ldddfloosuyuabb77EzweAlZEACrAS6Ny5c9q2bZspU6Ys0fXvvPNOkqR79+6LnOvRo0fD+Y+tscYai1xXV1eX2bNnL0O1i7feeuvlD3/4Q7p06ZJhw4ZlvfXWy3rrrZef/OQnn/i8d955p+Ln+Pj8v/r3z/Lx/bJL81lKpVIOO+yw/OIXv8hll12WDTfcMDvssMNir/3zn/+cwYMHJ/loS/Ef//jHPProoxk5cuRSv+/iPucn1XjooYdmzpw56datm3s/AWgSBFCAlUDz5s2z00475fHHH19kidDifBzC3nzzzUXOvfHGG+ncuXPVamvdunWSZO7cuY2O//t9pkmyww475Lbbbsv06dPzyCOPZNttt83w4cNz4403Vnz9NdZYo+LnSFLVz/KvDj300Lz99tu57LLLcthhh1W87sYbb0zLli1z++23Z//998+AAQOy9dZbL9N7Lm6ZUyVvvvlmhg0bli233DLvvPNOvvvd7y7TewLAykQABVhJjBgxIuVyOUcdddRil/bMnz8/t912W5Lky1/+cpI0uhcySR599NE8++yz2WmnnapW18ebXP/2t781Ov5xLYvTvHnz9O/fP5dcckmS5C9/+UvFa3faaafce++9DYHzYz/72c/Stm3bFfYVJWuuuWZOOeWU7LXXXjnkkEMqXlcqldKiRYs0b9684djs2bPz85//fJFrq9VVXrhwYb7xjW+kVCrlzjvvzOjRo3PRRRfl5ptvXu7XBoBasoQIYCWx7bbbZty4cRk6dGj69euX4447Lptttlnmz5+fJ554IldccUX69OmTvfbaKxtttFGOPvroXHTRRWnWrFl22223vPTSSzn99NPTs2fPnHjiiVWra/fdd0+nTp1yxBFH5JxzzkmLFi0yfvz4vPrqq42uu+yyy3Lvvfdmjz32yNprr505c+Y0bJrdeeedK77+mWeemdtvvz077rhjzjjjjHTq1CnXX399fve732XMmDGNFhBV27nnnvup1+yxxx654IILcuCBB+boo4/OO++8k/PPP3+xX5Wz+eab58Ybb8yvfvWr9O7dO61bt16m+zbPPPPMPPjgg7n77rvTrVu3nHzyyZk4cWKOOOKI9O3bd4mXVQHAykYABViJHHXUUdlmm21y4YUX5rzzzsvUqVPTsmXLbLjhhjnwwANz/PHHN1w7bty4rLfeern66qtzySWXpGPHjvnKV76S0aNHL/aez2XVoUOH3HXXXRk+fHi++c1vZvXVV8+RRx6Z3XbbLUceeWTDdVtuuWXuvvvunHnmmZk6dWpWW2219OnTJ7feemvDPZSLs9FGG2XSpEn5/ve/n2HDhmX27NnZZJNNcu211y6yZKkWvvzlL+eaa67Jeeedl7322itrrrlmjjrqqHTp0iVHHHFEo2vPPvvsvPnmmznqqKPywQcfZJ111mn0PalLYsKECRk9enROP/30Rp3s8ePHp2/fvjnggAPy0EMPpVWrVtX4eABQqFL5X79FGwAAAFYQ94ACAABQCAEUAACAQgigAAAAFEIABQAAoBACKAAAAIUQQAEAACiEAAoAAEAhWtS6gBVh1vlHfvpFwCqpw/fvrHUJAMBSWjDv9VqXsMzmv/1irUtYrJade9e6hGWiAwoAAEAhBFAAAAAK0SRHcAEAAKqifmGtK2hSdEABAAAohAAKAABAIYzgAgAAVFKur3UFTYoOKAAAAIUQQAEAACiEEVwAAIBK6o3gVpMOKAAAAIUQQAEAACiEEVwAAIAKyrbgVpUOKAAAAIUQQAEAACiEEVwAAIBKbMGtKh1QAAAACiGAAgAAUAgjuAAAAJXYgltVOqAAAAAUQgAFAACgEEZwAQAAKqlfWOsKmhQdUAAAAAohgAIAAFAII7gAAACV2IJbVTqgAAAAFEIABQAAoBBGcAEAACqpN4JbTTqgAAAAFEIABQAAoBBGcAEAACoo24JbVTqgAAAAFEIABQAAoBBGcAEAACqxBbeqdEABAAAohAAKAABAIYzgAgAAVGILblXpgAIAAFAIARQAAIBCGMEFAACopH5hrStoUnRAAQAAKIQACgAAQCGM4AIAAFRiC25V6YACAABQCAEUAACAQhjBBQAAqKTeCG416YACAABQCAEUAACAQhjBBQAAqMQW3KrSAQUAAKAQAigAAACFMIILAABQiS24VaUDCgAAQCEEUAAAAAphBBcAAKCCcnlhrUtoUnRAAQAAKIQACgAAQCGM4AIAAFRStgW3mnRAAQAAKIQACgAAQCGM4AIAAFRSbwS3mnRAAQAAKIQACgAAQCGM4AIAAFRiC25V6YACAABQCAEUAACAQhjBBQAAqKR+Ya0raFJ0QAEAACiEAAoAAEAhjOACAABUYgtuVemAAgAAUAgBFAAAgEIYwQUAAKik3ghuNemAAgAAUAgBFAAAgEIYwQUAAKjEFtyq0gEFAACgEAIoAAAAhTCCCwAAUIktuFWlAwoAAEAhBFAAAAAKYQQXAACgEiO4VaUDCgAAQCEEUAAAAAphBBcAAKCCcnlhrUtoUnRAAQAAKIQACgAAQCGM4AIAAFRiC25V6YACAABQCAEUAACAQhjBBQAAqKRsBLeadEABAAAohAAKAABAIYzgAgAAVGILblXpgAIAAFAIARQAAIBCGMEFAACoxBbcqtIBBQAAoBACKAAAAIUQQAEAACqpr185H0vpgQceyF577ZUePXqkVCrlN7/5TaPz5XI5Z511Vnr06JE2bdpk0KBBefrppxtdM3fu3Hz7299O586d065du+y999557bXXlqoOARQAAKCJmzlzZrbYYotcfPHFiz0/ZsyYXHDBBbn44ovz6KOPplu3btlll13ywQcfNFwzfPjw3HLLLbnxxhvz0EMP5cMPP8yee+6ZhQsXLnEdlhABAAA0cbvttlt22223xZ4rl8sZO3ZsRo4cmX333TdJct1116Vr16654YYbcswxx2T69Om5+uqr8/Of/zw777xzkuQXv/hFevbsmT/84Q/Zddddl6gOHVAAAIBKyvUr5WPu3LmZMWNGo8fcuXOX6SNOmTIlU6dOzeDBgxuO1dXVZeDAgZk0aVKS5PHHH8/8+fMbXdOjR4/06dOn4ZolIYACAACsYkaPHp2OHTs2eowePXqZXmvq1KlJkq5duzY63rVr14ZzU6dOTatWrfK5z32u4jVLwgguAADAKmbEiBE56aSTGh2rq6tbrtcslUqNfi6Xy4sc+3dLcs2/EkABAAAqWYaNs0Woq6tb7sD5sW7duiX5qMvZvXv3huPTpk1r6Ip269Yt8+bNy3vvvdeoCzpt2rQMGDBgid/LCC4AAMBnWK9evdKtW7dMmDCh4di8efMyceLEhnDZr1+/tGzZstE1b775Zv7+978vVQDVAQUAAGjiPvzww7zwwgsNP0+ZMiVPPvlkOnXqlLXXXjvDhw/PqFGjssEGG2SDDTbIqFGj0rZt2xx44IFJko4dO+aII47IySefnDXWWCOdOnXKd7/73Wy++eYNW3GXhAAKAABQyUo6gru0Hnvssey4444NP398/+ghhxyS8ePH59RTT83s2bMzdOjQvPfee+nfv3/uvvvutG/fvuE5F154YVq0aJH9998/s2fPzk477ZTx48enefPmS1xHqVwul6v3sVYOs84/stYlACtIh+/fWesSAICltGDe67UuYZnN/t3YWpewWG32GF7rEpaJe0ABAAAohBFcAACASspNYwR3ZaEDCgAAQCEEUAAAAAphBBcAAKCSJrIFd2WhAwoAAEAhBFAAAAAKYQQXAACgEltwq0oHFAAAgEIIoAAAABTCCC4AAEAltuBWlQ4oAAAAhRBAAQAAKIQRXFZ+pWZpOWDvNN+0f0ptO6Y8c3oWPP3HLHj4d0nKH13Tsi4tv/TVNF9/y5Rar5byjHey4C/3ZMFf769l5cAyOObog3PMMd/Kuuv0TJI888zz+dGPL8xdv7+vxpUBy8vfN6skW3CrSgBlpddim93SYouBmXvXNSm//UaadVs3rb5yWDJ3dhb85Z4kSasdD0iznhtn3h1Xpzz97TRbd7O02vmglD98Pwv/+WRtPwCwVF5//c2MHDk6L/zzpSTJwd/6Wm7+9TXZeptd88wzz9e2OGC5+PsGBFBWes179M7Cfz6Z+hefSpIsnPFOFm68TZp1XbfhmmY91suCpyel/tXJH13ztwdS/sLANOu2jgAKq5jbfzeh0c+nn3Fejjn6W+m/zVb+Dyqs4vx9AzUNoK+99lrGjRuXSZMmZerUqSmVSunatWsGDBiQY489Nj179qxleawkFr7+QlpsMTClz3VN+b3/Tenza6X5mhtk3n03/t81r/0jzdffIgv//lDKH76fZj03SqlT1yy87+kaVg4sr2bNmmW//fZMu3Zt88ifHq91OUAV+ftmlWELblXVLIA+9NBD2W233dKzZ88MHjw4gwcPTrlczrRp0/Kb3/wmF110Ue68885st912n/g6c+fOzdy5cxsdW7hgYepaNF+R5VOgBX++M6W6Nml9+A8/+i+AZs0y/8FbsvC5PzdcM//eX6bVroekzbHnp7xwQVIuZ97d16X+9RdqWDmwrPr02TgPPXBrWreuy4cfzsx+Xzsyzz77j1qXBVSBv2/4bKtZAD3xxBNz5JFH5sILL6x4fvjw4Xn00Uc/8XVGjx6ds88+u9Gx7+/SNyMHb1W1Wqmt5ht9Mc03+Y/Mu/3K1L/zRpp16ZlWO3495ZnTs/DpSUmSFlvtlGbde2fuzRelfsY7ad5zg7Ta+ZuZ++H01L/ybI0/AbC0Jk/+Z/p9cXBW79gh++67e665emy+vPNX/Z9UaAL8fcNnW6lcLpdr8cZt2rTJk08+mY022mix55977rn07ds3s2fP/sTXWWwH9NITdECbkNZHj8mCP9+ZBU/+34a8Fv+xR1ps8h+Zc+3pSYuWafPtizL3t5c03CeaJK0GH5JS+89l7q/H1qBqVpQO37+z1iVQA7+/88b888WXM3TYabUuBagyf9+fDQvmvV7rEpbZ7JvOqXUJi9Vm/zNqXcIyqVkHtHv37pk0aVLFAPrwww+ne/fun/o6dXV1qaura3RslvDZpJRatkr+/d+T1NcnpdJH/9yseUrNWyxyTblcn9LH1wCrtFKplLq6VrUuA1gB/H3DZ0vNAuh3v/vdHHvssXn88cezyy67pGvXrimVSpk6dWomTJiQq666KmPHjq1VeaxEFv7zr2nxH7un/oN3Pvoali5rp+XWg7Pg7w99dMG8OVn46uS0Gvi1zFswP+UZ76TZWhumxabbZv79N9W2eGCp/eiH38tdd92bV197I+3br5YD9t8nAwdumz32PKjWpQHLyd83ULMAOnTo0Kyxxhq58MILc/nll2fhwoVJkubNm6dfv3752c9+lv33379W5bESmXfPDWm5/ZC02vmbKbVpn/LM97PgrxMz/+HbGq6Ze9vlafWlr6bV7kem1LpdyjPeyfyHbsmCv95fu8KBZdKlS+eMv/an6d69S6ZP/yBPPfVs9tjzoPzhngdrXRqwnPx9s0qqzR2LTVbN7gH9V/Pnz8/bb7+dJOncuXNatmy5XK836/wjq1EWsBJyDygArHpW6XtAf3X2p19UA20OOLPWJSyTmn4P6Mdatmy5RPd7AgAAsOpaKQIoAADASqm+vtYVNCnNal0AAAAAnw0CKAAAAIUwggsAAFCJEdyq0gEFAACgEAIoAAAAhTCCCwAAUEnZCG416YACAABQCAEUAACAQhjBBQAAqMQW3KrSAQUAAKAQAigAAACFMIILAABQSblc6wqaFB1QAAAACiGAAgAAUAgjuAAAAJXYgltVOqAAAAAUQgAFAACgEEZwAQAAKjGCW1U6oAAAABRCAAUAAKAQRnABAAAqKRvBrSYdUAAAAAohgAIAAFAII7gAAAAVlOvLtS6hSdEBBQAAoBACKAAAAIUwggsAAFBJvS241aQDCgAAQCEEUAAAAAphBBcAAKCSshHcatIBBQAAoBACKAAAAIUwggsAAFBJfbnWFTQpOqAAAAAUQgAFAACgEEZwAQAAKqm3BbeadEABAAAohAAKAABAIYzgAgAAVGIEt6p0QAEAACiEAAoAAEAhjOACAABUUi7XuoImRQcUAACAQgigAAAAFMIILgAAQCW24FaVDigAAACFEEABAAAohBFcAACASuptwa0mHVAAAAAKIYACAABQCCO4AAAAlZRtwa0mHVAAAAAKIYACAABQCCO4AAAAldiCW1U6oAAAABRCAAUAAKAQRnABAAAqKNfbgltNOqAAAAAUQgAFAACgEEZwAQAAKrEFt6p0QAEAACiEAAoAAEAhjOACAABUUrYFt5p0QAEAACiEAAoAAEAhjOACAABUYgtuVemAAgAAUAgBFAAAgEIYwQUAAKik3hbcatIBBQAAoBACKAAAAIUwggsAAFCJLbhVpQMKAABAIQRQAAAACmEEFwAAoJKyLbjVpAMKAABAIQRQAAAACmEEFwAAoBJbcKtKBxQAAIBCCKAAAAAUwgguAABABeV6W3CrSQcUAACAQgigAAAAFMIILgAAQCW24FaVDigAAACFEEABAAAohBFcAACASozgVpUOKAAAAIUQQAEAACiEEVwAAIBKyvW1rqBJ0QEFAACgEAIoAAAAhTCCCwAAUIktuFWlAwoAAEAhBFAAAAAKYQQXAACggrIR3KrSAQUAAKAQAigAAACFMIILAABQiRHcqtIBBQAAoBACKAAAQBO2YMGC/OAHP0ivXr3Spk2b9O7dO+ecc07q6+sbrimXyznrrLPSo0ePtGnTJoMGDcrTTz9d9VqM4AIAAFTyLyFtVXXeeeflsssuy3XXXZfNNtssjz32WA477LB07NgxJ5xwQpJkzJgxueCCCzJ+/PhsuOGG+dGPfpRddtklkydPTvv27atWiw4oAABAE/bwww9nn332yR577JF11103++23XwYPHpzHHnssyUfdz7Fjx2bkyJHZd99906dPn1x33XWZNWtWbrjhhqrWIoACAACsYubOnZsZM2Y0esydO3ex126//fa555578vzzzydJ/vrXv+ahhx7K7rvvniSZMmVKpk6dmsGDBzc8p66uLgMHDsykSZOqWrcACgAAUEl9eaV8jB49Oh07dmz0GD169GI/wmmnnZZvfOMb2XjjjdOyZcv07ds3w4cPzze+8Y0kydSpU5MkXbt2bfS8rl27NpyrFveAAgAArGJGjBiRk046qdGxurq6xV77q1/9Kr/4xS9yww03ZLPNNsuTTz6Z4cOHp0ePHjnkkEMariuVSo2eVy6XFzm2vARQAACAVUxdXV3FwPnvTjnllHzve9/L17/+9STJ5ptvnpdffjmjR4/OIYcckm7duiX5qBPavXv3hudNmzZtka7o8jKCCwAAUMlKMG672MdSmDVrVpo1axz9mjdv3vA1LL169Uq3bt0yYcKEhvPz5s3LxIkTM2DAgOX/Hf4LHVAAAIAmbK+99sqPf/zjrL322tlss83yxBNP5IILLsjhhx+e5KPR2+HDh2fUqFHZYIMNssEGG2TUqFFp27ZtDjzwwKrWIoACAAA0YRdddFFOP/30DB06NNOmTUuPHj1yzDHH5Iwzzmi45tRTT83s2bMzdOjQvPfee+nfv3/uvvvuqn4HaJKUyuXy0vVvVwGzzj+y1iUAK0iH799Z6xIAgKW0YN7rtS5hmc04Ztdal7BYHS7/fa1LWCbuAQUAAKAQAigAAACFcA8oAABAJUu5cZZPpgMKAABAIQRQAAAACmEEFwAAoBIjuFWlAwoAAEAhBFAAAAAKYQQXAACggrIR3KpqkgF0u/OfrXUJwAry4T3n1roEYAXpueePal0CACuYEVwAAAAK0SQ7oAAAAFVhBLeqdEABAAAohAAKAABAIYzgAgAAVFJf6wKaFh1QAAAACiGAAgAAUAgjuAAAABWUbcGtKh1QAAAACiGAAgAAUAgjuAAAAJUYwa0qHVAAAAAKIYACAABQCCO4AAAAldTXuoCmRQcUAACAQgigAAAAFMIILgAAQAVlW3CrSgcUAACAQgigAAAAFMIILgAAQCW24FaVDigAAACFEEABAAAohBFcAACACmzBrS4dUAAAAAohgAIAAFAII7gAAACV2IJbVTqgAAAAFEIABQAAoBBGcAEAACooG8GtKh1QAAAACiGAAgAAUAgjuAAAAJUYwa0qHVAAAAAKIYACAABQCCO4AAAAFdiCW106oAAAABRCAAUAAKAQRnABAAAqMYJbVTqgAAAAFEIABQAAoBBGcAEAACqwBbe6dEABAAAohAAKAABAIQRQAAAACuEeUAAAgArcA1pdOqAAAAAUQgAFAACgEEZwAQAAKjCCW106oAAAABRCAAUAAKAQRnABAAAqKZdqXUGTogMKAABAIQRQAAAACmEEFwAAoAJbcKtLBxQAAIBCCKAAAAAUwgguAABABeV6W3CrSQcUAACAQgigAAAAFMIILgAAQAW24FaXDigAAACFEEABAAAohBFcAACACsplW3CrSQcUAACAQgigAAAAFMIILgAAQAW24FaXDigAAACFEEABAAAohBFcAACACsr1tuBWkw4oAAAAhRBAAQAAKIQRXAAAgArK5VpX0LTogAIAAFAIARQAAIBCGMEFAACowBbc6tIBBQAAoBACKAAAAIUwggsAAFCBEdzq0gEFAACgEAIoAAAAhTCCCwAAUEG5XOsKmhYdUAAAAAohgAIAAFAII7gAAAAV2IJbXTqgAAAAFEIABQAAoBBGcAEAACool43gVpMOKAAAAIUQQAEAACiEEVwAAIAKyvW1rqBp0QEFAACgEAIoAAAAhTCCCwAAUEG9LbhVpQMKAABAIZaoA3rrrbcu8Qvuvffey1wMAAAATdcSBdAhQ4Ys0YuVSqUsXLhweeoBAABYaZSN4FbVEgXQ+nq7hwEAAFg+y3UP6Jw5c6pVBwAAAE3cUgfQhQsX5oc//GHWXHPNrLbaannxxReTJKeffnquvvrqqhcIAABQK+X60kr5WFUtdQD98Y9/nPHjx2fMmDFp1apVw/HNN988V111VVWLAwAAoOlY6gD6s5/9LFdccUUOOuigNG/evOH4F77whTz33HNVLQ4AAICmY4mWEP2r119/Peuvv/4ix+vr6zN//vyqFAUAALAyKJdrXUHTstQd0M022ywPPvjgIsf/+7//O3379q1KUQAAADQ9S90BPfPMM/Otb30rr7/+eurr63PzzTdn8uTJ+dnPfpbbb799RdQIAABAE7DUHdC99torv/rVr3LHHXekVCrljDPOyLPPPpvbbrstu+yyy4qoEQAAoCZqve22qW3BXeoOaJLsuuuu2XXXXatdCwAAAE3YMgXQJHnsscfy7LPPplQqZZNNNkm/fv2qWRcAAABNzFIH0Ndeey3f+MY38sc//jGrr756kuT999/PgAED8stf/jI9e/asdo0AAAA1UV9edcddV0ZLfQ/o4Ycfnvnz5+fZZ5/Nu+++m3fffTfPPvtsyuVyjjjiiBVRIwAAAE3AUndAH3zwwUyaNCkbbbRRw7GNNtooF110UbbbbruqFgcAAEDTsdQBdO211878+fMXOb5gwYKsueaaVSkKAABgZVA2gltVSz2CO2bMmHz729/OY489lnK5nOSjhUQnnHBCzj///KoXCAAAQNOwRB3Qz33ucymV/i/5z5w5M/3790+LFh89fcGCBWnRokUOP/zwDBkyZIUUCgAAwKptiQLo2LFjV3AZAAAAK5//N/RJlSxRAD3kkENWdB0AAAA0cUu9hOhfzZ49e5GFRB06dFiuggAAAGialjqAzpw5M6eddlpuuummvPPOO4ucX7hwYVUKAwAAqLV6W3Craqm34J566qm59957c+mll6auri5XXXVVzj777PTo0SM/+9nPVkSNAAAALIfXX3893/zmN7PGGmukbdu22XLLLfP44483nC+XyznrrLPSo0ePtGnTJoMGDcrTTz9d9TqWOoDedtttufTSS7PffvulRYsW2WGHHfKDH/wgo0aNyvXXX1/1AgEAAFh27733Xrbbbru0bNkyd955Z5555pn813/9V1ZfffWGa8aMGZMLLrggF198cR599NF069Ytu+yySz744IOq1rLUI7jvvvtuevXqleSj+z3ffffdJMn222+f4447rqrFAQAA1FK5CYzgnnfeeenZs2euvfbahmPrrrtuwz+Xy+WMHTs2I0eOzL777pskue6669K1a9fccMMNOeaYY6pWy1J3QHv37p2XXnopSbLpppvmpptuSvJRZ/RfEzQAAAArxty5czNjxoxGj7lz5y722ltvvTVbb711vva1r6VLly7p27dvrrzyyobzU6ZMydSpUzN48OCGY3V1dRk4cGAmTZpU1bqXOoAedthh+etf/5okGTFiRMO9oCeeeGJOOeWUqhYHAADAokaPHp2OHTs2eowePXqx17744osZN25cNthgg/z+97/Psccem+985zsNO3ymTp2aJOnatWuj53Xt2rXhXLUs9QjuiSee2PDPO+64Y5577rk89thjWW+99bLFFltUtTj42Fb/sUUOHnpgNv3Cxvl8t8458dDv5f67Hmw4/8TUPy72eReec0l+dukNRZUJLIHHn3854+96JM++/Gbemv5hLhz2tXy570YN58vlci679YH8+oEnMmPWnGzeq0dGHLRb1l/z8w3XnPOz3+VPz07JW+9/mLZ1rbLF+mtl+Fe/nF7dO9fiIwFLod1q7TJi5AnZfc+d0/nza+Spvz2Tkd8blSf/8lStS4PFKpdrXcHijRgxIieddFKjY3V1dYu9tr6+PltvvXVGjRqVJOnbt2+efvrpjBs3LgcffHDDdaVS43Hjcrm8yLHltdQd0H+39tprZ999902nTp1y+OGHV6MmWESbtm3y/NMv5NzvX7DY8ztvvlejx5nDf5z6+vrcc/v9xRYKfKrZc+dno55d8r0Dv7LY89fe9XB+PuFP+d6BX8n1Pzg8a3RcLcdecH1mzvm/saJN1+mecw7bK7f88NiMO/EbKZfLOfbCG7Kwvr6ojwEso7EX/SgDdxyQYcecmoED9sr99/4xv/7NtenWvUutS4NVSl1dXTp06NDoUSmAdu/ePZtuummjY5tsskleeeWVJEm3bt2SZJFu57Rp0xbpii6v5Q6gH3v33Xdz3XXXVevloJE/3vtILj3vytx7x8TFnn/nrXcbPQbtukMe/eNf8vorbxRcKfBptt98/Rz//+2YnfttvMi5crmc6//w5xy5x/bZud/G2WDNLvnR4Xtnzrz5ueNPf2+4br+BW6Xfhutkzc6rZ5N1uuf4IYMy9d0ZeePt9wv8JMDSat26LnvuPTjnnPGfeXjSY5ny4iv5z3Mvzisvv5bDjjiw1uVBk7Xddttl8uTJjY49//zzWWeddZIkvXr1Srdu3TJhwoSG8/PmzcvEiRMzYMCAqtZStQAKK4tOnT+X7XcekN/ccHutSwGW0utvv5+3p3+YbTfr3XCsVcsW6bfROvnrC68t9jmz5s7Lb//416zZefV069SxqFKBZdC8RYu0aNEic/5tUcrsOXPS/z+2qlFV8Mnqy6WV8rE0TjzxxDzyyCMZNWpUXnjhhdxwww254oorMmzYsCQfjd4OHz48o0aNyi233JK///3vOfTQQ9O2bdsceGB1/+XQUt8DCiu7vQ7YLbM+nFWxWwqsvN6e/mGSZI0O7RodX6NDu7zxzvRGx35132O58H/uyey589Or2xq5/KQD07JF88JqBZbezA9n5s9/+ktOPmVonp/8Yt6a9nb23W/P9Nt6i7z4z5drXR40WV/84hdzyy23ZMSIETnnnHPSq1evjB07NgcddFDDNaeeempmz56doUOH5r333kv//v1z9913p3379lWtZaXugL766qufel/p4tYP15fdA/RZts/X98ydN9+deXPn1boUYBn9+7/XLZfLixzbvX+f/OqMo3LNqd/K2l075ZTLbs7c+QuKKhFYRsOOOTWlUil/n/xgXn/rqRx17Lfy6/++PQsXLqx1adCk7bnnnnnqqacyZ86cPPvssznqqKManS+VSjnrrLPy5ptvZs6cOZk4cWL69OlT9TqWuAP68ReSVvL+++8vby2L+Pi+0muuuabiNaNHj87ZZ5/d6FjXdmul+2prV70eVn59+2+RXhusk+8dc0atSwGWQeeOqyVJ3p4xM59f/f/+jeu7H8xapCvavm3rtG/bOut07ZQv9F4r23/n/Nz7l+eyW//q/48lUD0vTXk1++zxrbRt2ybt26+W//3ft3LltRfmlZcXP2YPtVZeynFXPtkSB9COHT/5vpqOHTs2WuG7JG699dZPPP/iiy9+6mssbv3wDhvsulR10HQMOXDPPPPX5/L8My/UuhRgGazZefV07rhaHnn6xWyy9kcb+eYvWJjHJ7+cE/b78qc8u5x5C3RQYFUxa9bszJo1Ox1X75Adv7x9zj7zP2tdElCAJQ6g1157bdXffMiQISmVSil/wpfrfNr3ztTV1S2ybrhZaaWeLGYZtGnbJj17rdXw85pr98iGm22QGe/PyNTX/zdJ0m61ttllrx1zwVkX16pMYAnMmjMvr0x7t+Hn1996P8+9MjUd27VJ9zU65qCdt8nVd/wxa3ftlLW7dsrVv/tjWrdqmd3/X2fztbfey+8ffSbbbto7n2vfNtPe/yDX3jkpdS1bZvvN16/VxwKW0I47bZ9SSnnhhSnp1XvtnHXOqXnhhSn55S9urnVpQAFquoSoe/fuueSSSzJkyJDFnn/yySfTr1+/YotipbTplhvnqpv/L1h+95zvJElu/dUdOfOEHydJdh2yc5JS7rplwuJeAlhJPP3SGzny/F80/Hz+TR/9ze494Av54eF757CvbJu58+Zn1PV3ZcbM2dm895oZd9KBadf6o3/Z2Kpli/zl+Vfyiwl/zoxZs7NGh3bpt+Ha+dmIQxcZ0wVWPh06tM/IM09Kjx7d8v577+f2W+/Oj394YRYscA83K6el3TjLJyuVP6n9uILtvffe2XLLLXPOOecs9vxf//rX9O3bN/VL+cXifbttV43ygJXQw/99bK1LAFaQnnv+qNYlACvIW9Mnf/pFK6k/9fjkXTi10v+NVXNqoKYd0FNOOSUzZ86seH799dfPfffdV2BFAAAArCg1DaA77LDDJ55v165dBg4cWFA1AAAAjdVsXLSJsq0HAACAQixTAP35z3+e7bbbLj169MjLL7+cJBk7dmx++9vfVrU4AAAAmo6lDqDjxo3LSSedlN133z3vv/9+Fi786DvXVl999YwdO7ba9QEAANRMfbm0Uj5WVUsdQC+66KJceeWVGTlyZJo3b95wfOutt85TTz1V1eIAAABoOpY6gE6ZMiV9+/Zd5HhdXd0nbrQFAADgs22pt+D26tUrTz75ZNZZZ51Gx++8885suummVSsMAACg1sqr8LjrymipA+gpp5ySYcOGZc6cOSmXy/nzn/+cX/7ylxk9enSuuuqqFVEjAAAATcBSB9DDDjssCxYsyKmnnppZs2blwAMPzJprrpmf/OQn+frXv74iagQAAKAJWOoAmiRHHXVUjjrqqLz99tupr69Ply5dql0XAABAzdXXuoAmZpkC6Mc6d+5crToAAABo4pZpCVGpVPlG3BdffHG5CgIAAKBpWuoAOnz48EY/z58/P0888UTuuuuunHLKKdWqCwAAoObKsQW3mpY6gJ5wwgmLPX7JJZfkscceW+6CAAAAaJqaVeuFdtttt/z617+u1ssBAADQxCzXEqJ/9T//8z/p1KlTtV4OAACg5urLta6gaVnqANq3b99GS4jK5XKmTp2at956K5deemlViwMAAKDpWOoAOmTIkEY/N2vWLJ///OczaNCgbLzxxtWqCwAAgCZmqQLoggULsu6662bXXXdNt27dVlRNAAAAK4V6W3CraqmWELVo0SLHHXdc5s6du6LqAQAAoIla6i24/fv3zxNPPLEiagEAAKAJW+p7QIcOHZqTTz45r732Wvr165d27do1Ov+FL3yhasUBAADUUtkIblUtcQA9/PDDM3bs2BxwwAFJku985zsN50qlUsrlckqlUhYuXFj9KgEAAFjlLXEAve6663LuuedmypQpK7IeAAAAmqglDqDl8kffwLrOOuussGIAAABWJvW1LqCJWaolRKWS+WcAAACWzVItIdpwww0/NYS+++67y1UQAAAATdNSBdCzzz47HTt2XFG1AAAArFRswa2upQqgX//619OlS5cVVQsAAABN2BLfA+r+TwAAAJbHUm/BBQAA+KywBbe6ljiA1tf71QMAALDsluprWAAAAGBZLdUSIgAAgM8Sc6DVpQMKAABAIQRQAAAACmEEFwAAoIJyfB1lNemAAgAAUAgBFAAAgEIYwQUAAKig3gRuVemAAgAAUAgBFAAAgEIYwQUAAKig3hbcqtIBBQAAoBACKAAAAIUwggsAAFBBudYFNDE6oAAAABRCAAUAAKAQRnABAAAqqK91AU2MDigAAACFEEABAAAohBFcAACACupLpVqX0KTogAIAAFAIARQAAIBCGMEFAACooFzrApoYHVAAAAAKIYACAABQCCO4AAAAFdTXuoAmRgcUAACAQgigAAAAFMIILgAAQAX1pVpX0LTogAIAAFAIARQAAIBCGMEFAACooD5mcKtJBxQAAIBCCKAAAAAUwgguAABABeVaF9DE6IACAABQCAEUAACAQhjBBQAAqKDeEtyq0gEFAACgEAIoAAAAhTCCCwAAUEF9rQtoYnRAAQAAKIQACgAAQCGM4AIAAFRQrnUBTYwOKAAAAIUQQAEAACiEEVwAAIAK6ku1rqBp0QEFAACgEAIoAAAAhTCCCwAAUEF9rQtoYnRAAQAAKIQACgAAQCGM4AIAAFRgBLe6dEABAAAohAAKAABAIYzgAgAAVFAu1bqCpkUHFAAAgEIIoAAAABTCCC4AAEAFtuBWlw4oAAAAhRBAAQAAKIQRXAAAgAqM4FaXDigAAACFEEABAAAohBFcAACACsq1LqCJ0QEFAACgEAIoAAAAhTCCCwAAUEF9qdYVNC06oAAAABRCAAUAAKAQRnABAAAqqK91AU2MDigAAACFEEABAAAohBFcAACACozgVpcOKAAAAIUQQAEAACiEEVwAAIAKyrUuoInRAQUAAKAQAigAAACFMIILAABQQX2p1hU0LTqgAAAAFEIABQAAoBBGcAEAACqor3UBTYwOKAAAAIUQQAEAACiEEVwAAIAKyrUuoInRAQUAAKAQAigAAACFEEABAAAqqE95pXwsj9GjR6dUKmX48OENx8rlcs4666z06NEjbdq0yaBBg/L0008v529vUQIoAADAZ8Sjjz6aK664Il/4whcaHR8zZkwuuOCCXHzxxXn00UfTrVu37LLLLvnggw+q+v5NcgnRU+++VOsSgBWkx+7n1LoEYAX53ym/r3UJAE3ahx9+mIMOOihXXnllfvSjHzUcL5fLGTt2bEaOHJl99903SXLdddela9euueGGG3LMMcdUrQYdUAAAgArqV9LH3LlzM2PGjEaPuXPnfuJnGTZsWPbYY4/svPPOjY5PmTIlU6dOzeDBgxuO1dXVZeDAgZk0adJS/sY+mQAKAACwihk9enQ6duzY6DF69OiK19944415/PHHF3vN1KlTkyRdu3ZtdLxr164N56qlSY7gAgAANGUjRozISSed1OhYXV3dYq999dVXc8IJJ+Tuu+9O69atK75mqVRq9HO5XF7k2PISQAEAACpYvn2zK05dXV3FwPnvHn/88UybNi39+vVrOLZw4cI88MADufjiizN58uQkH3VCu3fv3nDNtGnTFumKLi8juAAAAE3YTjvtlKeeeipPPvlkw2PrrbfOQQcdlCeffDK9e/dOt27dMmHChIbnzJs3LxMnTsyAAQOqWosOKAAAQBPWvn379OnTp9Gxdu3aZY011mg4Pnz48IwaNSobbLBBNthgg4waNSpt27bNgQceWNVaBFAAAIAK6mtdQEFOPfXUzJ49O0OHDs17772X/v375+6770779u2r+j6lcrm8so41L7MWrdasdQnACtKhrm2tSwBWEN8DCk1Xy869a13CMjtrnYNqXcJinfXy9bUuYZm4BxQAAIBCGMEFAACooL6630LymacDCgAAQCEEUAAAAAphBBcAAKCC+jS5na01pQMKAABAIQRQAAAACmEEFwAAoAIDuNWlAwoAAEAhBFAAAAAKYQQXAACggvpaF9DE6IACAABQCAEUAACAQhjBBQAAqKDeHtyq0gEFAACgEAIoAAAAhTCCCwAAUIEB3OrSAQUAAKAQAigAAACFMIILAABQQX2tC2hidEABAAAohAAKAABAIYzgAgAAVFBvD25V6YACAABQCAEUAACAQhjBBQAAqMAAbnXpgAIAAFAIARQAAIBCGMEFAACooL7WBTQxOqAAAAAUQgAFAACgEEZwAQAAKijbg1tVOqAAAAAUQgAFAACgEEZwAQAAKrAFt7p0QAEAACiEAAoAAEAhjOACAABUUG8LblXpgAIAAFAIARQAAIBCGMEFAACowABudemAAgAAUAgBFAAAgEIYwQUAAKjAFtzq0gEFAACgEAIoAAAAhTCCCwAAUEF9rQtoYnRAAQAAKIQACgAAQCGM4AIAAFRQtgW3qnRAAQAAKIQACgAAQCGM4AIAAFRgC2516YACAABQCAEUAACAQhjBBQAAqMAW3OrSAQUAAKAQAigAAACFMIILAABQgS241aUDCgAAQCEEUAAAAAphBBcAAKCC+rItuNWkAwoAAEAhBFAAAAAKYQQXAACgAgO41aUDCgAAQCEEUAAAAAphBBcAAKCCekO4VaUDCgAAQCEEUAAAAAphBBcAAKCCshHcqtIBBQAAoBACKAAAAIUwggsAAFBBfa0LaGJ0QAEAACiEAAoAAEAhjOACAABUUG8LblXpgAIAAFAIARQAAIBCGMEFAACooGwEt6p0QAEAACiEAAoAAEAhjOACAABUUF/rApoYHVAAAAAKIYACAABQCCO4AAAAFZTLtuBWkw4oAAAAhRBAAQAAKIQRXAAAgArqYwS3mnRAAQAAKIQACgAAQCGM4AIAAFRQX+sCmhgdUAAAAAohgAIAAFAII7gAAAAVlG3BrSodUAAAAAohgAIAAFAII7gAAAAV1BvBrSodUAAAAAohgAIAAFAII7gAAAAVlMtGcKtJBxQAAIBCCKAAAAAUwgguAABABfW1LqCJ0QEFAACgEAIoAAAAhTCCCwAAUEE5tuBWkw4oAAAAhRBAAQAAKIQRXAAAgArqjeBWlQ4oAAAAhRBAAQAAKIQRXAAAgArKZSO41aQDCgAAQCEEUFZJxxx9cP7y+IS8+/Zzefft5/LQA7fmK7vuWOuygCpo3rx5vn/6iXniqXvz+rSn8pe/3ZtTTjs+pVKp1qUBn+KxJ5/KsFPPzI57H5Q+2+2Wex6Y1Oj8hPv/mKNPHJntdz8gfbbbLc89/89FXuPQ409Nn+12a/T47hmji/oIwApmBJdV0uuvv5mRI0fnhX++lCQ5+Ftfy82/viZbb7Nrnnnm+doWByyXE048Oocd8fUMPea0PPfsP9K37+a5aNzozJjxQS4fd12tywM+wezZc7LR+r0zZPfBOXHkjxY9P2dO+m6+aQbvuEPOOu8nFV9nv72/kuOP/FbDz3V1dSukXlgStuBWlwDKKun2301o9PPpZ5yXY47+Vvpvs5UACqu4L/bvmzt/d08m/P7+JMmrr7yer35tz2zZt09tCwM+1Q7bfjE7bPvFiuf3/spOSZLX3/zfT3yd1nV16bxGp6rWBqwcjOCyymvWrFn233/vtGvXNo/86fFalwMsp0cefixfGrht1lt/3STJZn02Tv9t+2XC3RNrWxhQmN9NuC/b735A9jnomPznxVdm5sxZtS4JqJKad0Bnz56dxx9/PJ06dcqmm27a6NycOXNy00035eCDD674/Llz52bu3LmNjpXLZfcKfQb06bNxHnrg1rRuXZcPP5yZ/b52ZJ599h+1LgtYTj+54Ip06NA+f3r891m4cGGaN2+eH51zQW7+n9trXRpQgD0H75g1u3dL5zU+l3+8+FJ+ctn4TP7HlFz1k1G1Lo3PqLIR3KqqaQf0+eefzyabbJIvfelL2XzzzTNo0KC8+eabDeenT5+eww477BNfY/To0enYsWOjR7n+gxVdOiuByZP/mX5fHJzttt8rl1/xs1xz9dhssskGtS4LWE77fnWP7H/APjn68JMyaPshGXrMqTn+O0fk6wf+f7UuDSjAfnvvlm2/2Dcb9F43u+88KBf8aGQeeeyJPDP5hVqXBlRBTQPoaaedls033zzTpk3L5MmT06FDh2y33XZ55ZVXlvg1RowYkenTpzd6lJq1X4FVs7KYP39+/vnPl/L4X/6WkT84N3/72zP59vFH1rosYDmd/aPTMvaCy3Pzr3+XZ595Pjfd+NuMu3h8hp98TK1LA2pg043WT4sWLfLyq6/XuhSgCmo6gjtp0qT84Q9/SOfOndO5c+fceuutGTZsWHbYYYfcd999adeu3ae+Rl1d3SKb0YzffjaVSqXU1bWqdRnAcmrTtnXq6xuPOy2sX5hmzawtgM+iF6a8nAULFuTznS0lojbqy0Zwq6mmAXT27Nlp0aJxCZdcckmaNWuWgQMH5oYbbqhRZazsfvTD7+Wuu+7Nq6+9kfbtV8sB+++TgQO3zR57HlTr0oDldNed9+XkU47La6+9keee/Ue+sMWmGXr84bn+5/9T69KATzFr1uy88tobDT+//sb/5rnn/5mOHdqne7cumT7jg7w5dVqmvf1OkmTKK68lSTqv8bl0XqNTXnntjfzu7vuyw7ZfzOdW75h/Tnk5/3nxVdlkw/XSd/NNF/uewKqlpgF04403zmOPPZZNNtmk0fGLLroo5XI5e++9d40qY2XXpUvnjL/2p+nevUumT/8gTz31bPbY86D84Z4Ha10asJy+991z8v0fDM/5F5yVzp9fI1PfnJbx19yY/zz34lqXBnyKvz/3jxz+7dMafh5z0RVJkn122zk//sHJue/BR/KDURc0nD/lzHOTJMcdflCGHfHNtGzZMn96/Mn84r9/m1mzZ6dbl8/nSwO2ydDDD0rz5s2L/TDAClEql2vXUx49enQefPDB3HHHHYs9P3To0Fx22WWpr69fqtdt0WrNapQHrIQ61LWtdQnACvK/U35f6xKAFaRl5961LmGZ7bDmTrUuYbEefP2eWpewTGoaQFcUARSaLgEUmi4BFJouAbT6VtUAaqMDAAAAhRBAAQAAKqhPeaV8LI3Ro0fni1/8Ytq3b58uXbpkyJAhmTx5cqNryuVyzjrrrPTo0SNt2rTJoEGD8vTTT1fzV5lEAAUAAGjSJk6cmGHDhuWRRx7JhAkTsmDBggwePDgzZ85suGbMmDG54IILcvHFF+fRRx9Nt27dsssuu+SDDz6oai3uAQVWKe4BhabLPaDQdK3K94But+aXa13CYv3x9XuX+blvvfVWunTpkokTJ+ZLX/pSyuVyevTokeHDh+e00z7aZD137tx07do15513Xo455phqla0DCgAAUEmtR20rPebOnZsZM2Y0esydO3eJPtP06dOTJJ06dUqSTJkyJVOnTs3gwYMbrqmrq8vAgQMzadKkqv4+BVAAAIBVzOjRo9OxY8dGj9GjR3/q88rlck466aRsv/326dOnT5Jk6tSpSZKuXbs2urZr164N56qlRVVfDQAAgBVuxIgROemkkxodq6ur+9TnHX/88fnb3/6Whx56aJFzpVKp0c/lcnmRY8tLAAUAAKhgZV2ZU1dXt0SB8199+9vfzq233poHHngga621VsPxbt26JfmoE9q9e/eG49OmTVukK7q8jOACAAA0YeVyOccff3xuvvnm3HvvvenVq1ej87169Uq3bt0yYcKEhmPz5s3LxIkTM2DAgKrWogMKAADQhA0bNiw33HBDfvvb36Z9+/YN93V27Ngxbdq0SalUyvDhwzNq1KhssMEG2WCDDTJq1Ki0bds2Bx54YFVrEUABAAAqqM/KOYK7NMaNG5ckGTRoUKPj1157bQ499NAkyamnnprZs2dn6NChee+999K/f//cfffdad++fVVr8T2gwCrF94BC0+V7QKHpWpW/B3SbHgNrXcJi/fmNibUuYZm4BxQAAIBCGMEFAACooNwERnBXJjqgAAAAFEIABQAAoBBGcAEAACpogjtba0oHFAAAgEIIoAAAABTCCC4AAEAF9bbgVpUOKAAAAIUQQAEAACiEEVwAAIAKbMGtLh1QAAAACiGAAgAAUAgjuAAAABXYgltdOqAAAAAUQgAFAACgEEZwAQAAKigbwa0qHVAAAAAKIYACAABQCCO4AAAAFdSXjeBWkw4oAAAAhRBAAQAAKIQRXAAAgApswa0uHVAAAAAKIYACAABQCAEUAACAQrgHFAAAoAJfw1JdOqAAAAAUQgAFAACgEEZwAQAAKvA1LNWlAwoAAEAhBFAAAAAKYQQXAACgAltwq0sHFAAAgEIIoAAAABTCCC4AAEAFtuBWlw4oAAAAhRBAAQAAKIQRXAAAgApswa0uHVAAAAAKIYACAABQCCO4AAAAFdiCW106oAAAABRCAAUAAKAQRnABAAAqKJfra11Ck6IDCgAAQCEEUAAAAAphBBcAAKCCeltwq0oHFAAAgEIIoAAAABTCCC4AAEAF5bIR3GrSAQUAAKAQAigAAACFMIILAABQgS241aUDCgAAQCEEUAAAAAphBBcAAKACW3CrSwcUAACAQgigAAAAFMIILgAAQAX1RnCrSgcUAACAQgigAAAAFMIILgAAQAXlGMGtJh1QAAAACiGAAgAAUAgjuAAAABWUbcGtKh1QAAAACiGAAgAAUAgjuAAAABXU24JbVTqgAAAAFEIABQAAoBBGcAEAACqwBbe6dEABAAAohAAKAABAIYzgAgAAVFBvBLeqdEABAAAohAAKAABAIYzgAgAAVGALbnXpgAIAAFAIARQAAIBCGMEFAACooD5GcKtJBxQAAIBCCKAAAAAUwgguAABABbbgVpcOKAAAAIUQQAEAACiEEVwAAIAK6o3gVpUOKAAAAIUQQAEAACiEEVwAAIAKyjGCW006oAAAABRCAAUAAKAQRnABAAAqsAW3unRAAQAAKIQACgAAQCGM4AIAAFRQNoJbVTqgAAAAFEIABQAAoBBGcAEAACooxwhuNemAAgAAUAgBFAAAgEIYwQUAAKjAFtzq0gEFAACgEAIoAAAAhTCCCwAAUIER3OrSAQUAAKAQAigAAACFMIILAABQgQHc6tIBBQAAoBACKAAAAIUola11YhU2d+7cjB49OiNGjEhdXV2tywGqyN83NF3+vuGzSwBllTZjxox07Ngx06dPT4cOHWpdDlBF/r6h6fL3DZ9dRnABAAAohAAKAABAIQRQAAAACiGAskqrq6vLmWeeaYEBNEH+vqHp8vcNn12WEAEAAFAIHVAAAAAKIYACAABQCAEUAACAQgigAAAAFEIAZZV26aWXplevXmndunX69euXBx98sNYlAcvpgQceyF577ZUePXqkVCrlN7/5Ta1LAqpk9OjR+eIXv5j27dunS5cuGTJkSCZPnlzrsoACCaCssn71q19l+PDhGTlyZJ544onssMMO2W233fLKK6/UujRgOcycOTNbbLFFLr744lqXAlTZxIkTM2zYsDzyyCOZMGFCFixYkMGDB2fmzJm1Lg0oiK9hYZXVv3//bLXVVhk3blzDsU022SRDhgzJ6NGja1gZUC2lUim33HJLhgwZUutSgBXgrbfeSpcuXTJx4sR86UtfqnU5QAF0QFklzZs3L48//ngGDx7c6PjgwYMzadKkGlUFACyN6dOnJ0k6depU40qAogigrJLefvvtLFy4MF27dm10vGvXrpk6dWqNqgIAllS5XM5JJ52U7bffPn369Kl1OUBBWtS6AFgepVKp0c/lcnmRYwDAyuf444/P3/72tzz00EO1LgUokADKKqlz585p3rz5It3OadOmLdIVBQBWLt/+9rdz66235oEHHshaa61V63KAAhnBZZXUqlWr9OvXLxMmTGh0fMKECRkwYECNqgIAPkm5XM7xxx+fm2++Offee2969epV65KAgumAsso66aST8q1vfStbb711tt1221xxxRV55ZVXcuyxx9a6NGA5fPjhh3nhhRcafp4yZUqefPLJdOrUKWuvvXYNKwOW17Bhw3LDDTfkt7/9bdq3b98wydSxY8e0adOmxtUBRfA1LKzSLr300owZMyZvvvlm+vTpkwsvvNAad1jF3X///dlxxx0XOX7IIYdk/PjxxRcEVE2lPQ3XXnttDj300GKLAWpCAAUAAKAQ7gEFAACgEAIoAAAAhRBAAQAAKIQACgAAQCEEUAAAAAohgAIAAFAIARQAAIBCCKAAAAAUQgAFYJmcddZZ2XLLLRt+PvTQQzNkyJDC63jppZdSKpXy5JNPrrD3+PfPuiyKqBMAVnYCKEATcuihh6ZUKqVUKqVly5bp3bt3vvvd72bmzJkr/L1/8pOfZPz48Ut0bdFhbNCgQRk+fHgh7wUAVNai1gUAUF1f+cpXcu2112b+/Pl58MEHc+SRR2bmzJkZN27cItfOnz8/LVu2rMr7duzYsSqvAwA0XTqgAE1MXV1dunXrlp49e+bAAw/MQQcdlN/85jdJ/m+U9Jprrknv3r1TV1eXcrmc6dOn5+ijj06XLl3SoUOHfPnLX85f//rXRq977rnnpmvXrmnfvn2OOOKIzJkzp9H5fx/Bra+vz3nnnZf1118/dXV1WXvttfPjH/84SdKrV68kSd++fVMqlTJo0KCG51177bXZZJNN0rp162y88ca59NJLG73Pn//85/Tt2zetW7fO1ltvnSeeeGK5f2ennXZaNtxww7Rt2za9e/fO6aefnvnz5y9y3eWXX56ePXumbdu2+drXvpb333+/0flPqx0APut0QAGauDZt2jQKUy+88EJuuumm/PrXv07z5s2TJHvssUc6deqUO+64Ix07dszll1+enXbaKc8//3w6deqUm266KWeeeWYuueSS7LDDDvn5z3+en/70p+ndu3fF9x0xYkSuvPLKXHjhhdl+++3z5ptv5rnnnkvyUYjcZptt8oc//CGbbbZZWrVqlSS58sorc+aZZ+biiy9O375988QTT+Soo45Ku3btcsghh2TmzJnZc8898+Uvfzm/+MUvMmXKlJxwwgnL/Ttq3759xo8fnx49euSpp57KUUcdlfbt2+fUU09d5Pd22223ZcaMGTniiCMybNiwXH/99UtUOwCQpAxAk3HIIYeU99lnn4af//SnP5XXWGON8v77718ul8vlM888s9yyZcvytGnTGq655557yh06dCjPmTOn0Wutt9565csvv7xcLpfL2267bfnYY49tdL5///7lLbbYYrHvPWPGjHJdXV35yiuvXGydU6ZMKScpP/HEE42O9+zZs3zDDTc0OvbDH/6wvO2225bL5XL58ssvL3fq1Kk8c+bMhvPjxo1b7Gv9q4EDB5ZPOOGEiuf/3ZgxY8r9+vVr+PnMM88sN2/evPzqq682HLvzzjvLzZo1K7/55ptLVHulzwwAnyU6oABNzO23357VVlstCxYsyPz587PPPvvkoosuaji/zjrr5POf/3zDz48//ng+/PDDrLHGGo1eZ/bs2fnnP/+ZJHn22Wdz7LHHNjq/7bbb5r777ltsDc8++2zmzp2bnXbaaYnrfuutt/Lqq6/miCOOyFFHHdVwfMGCBQ33lz777LPZYost0rZt20Z1LK//+Z//ydixY/PCCy/kww8/zIIFC9KhQ4dG16y99tpZa621Gr1vfX19Jk+enObNm39q7QCAEVyAJmfHHXfMuHHj0rJly/To0WORJUPt2rVr9HN9fX26d++e+++/f5HXWn311ZephjZt2iz1c+rr65N8NMrav3//Ruc+HhUul8vLVM8neeSRR/L1r389Z599dnbdddd07NgxN954Y/7rv/7rE59XKpUa/nNJagcABFCAJqddu3ZZf/31l/j6rbbaKlOnTk2LFi2y7rrrLvaaTTbZJI888kgOPvjghmOPPPJIxdfcYIMN0qZNm9xzzz058sgjFzn/8T2fCxcubDjWtWvXrLnmmnnxxRdz0EEHLfZ1N9100/z85z/P7NmzG0LuJ9WxJP74xz9mnXXWyciRIxuOvfzyy4tc98orr+SNN95Ijx49kiQPP/xwmjVrlg033HCJagcABFCAz7ydd9452267bYYMGZLzzjsvG220Ud54443ccccdGTJkSLbeeuuccMIJOeSQQ7L11ltn++23z/XXX5+nn3664hKi1q1b57TTTsupp56aVq1aZbvttstbb72Vp59+OkcccUS6dOmSNm3a5K677spaa62V1q1bp2PHjjnrrLPyne98Jx06dMhuu+2WuXPn5rHHHst7772Xk046KQceeGBGjhyZI444Ij/4wQ/y0ksv5fzzz1+iz/nWW28t8r2j3bp1y/rrr59XXnklN954Y774xS/md7/7XW655ZbFfqZDDjkk559/fmbMmJHvfOc72X///dOtW7ck+dTaAQBfwwLwmVcqlXLHHXfkS1/6Ug4//PBsuOGG+frXv56XXnopXbt2TZIccMABOeOMM3LaaaelX79+efnll3Pcccd94uuefvrpOfnkk3PGGWdkk002yQEHHJBp06YlSVq0aJGf/vSnufzyy9OjR4/ss88+SZIjjzwyV111VcaPH5/NN988AwcOzPjx4xu+tmW11VbLbbfdlmeeeSZ9+/bNyJEjc9555y3R57zhhhvSt2/fRo/LLrss++yzT0488cQcf/zx2XLLLTNp0qScfvrpizx//fXXz7777pvdd989gwcPTp8+fRp9zcqn1Q4AJKXyirihBgAAAP6NDigAAACFEEABAAAohAAKAABAIQRQAAAACiGAAgAAUAgBFAAAgEIIoAAAABRCAAUAAKAQAigAAACFEEABAAAohAAKAABAIf5/MfnykgLf9VgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1200x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "cf_matrix = confusion_matrix(result,y_test)\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_test[34] # 2\n",
    "\n",
    "# y_test[54] # 1\n",
    "\n",
    "# y_test[89] # 0\n",
    "\n",
    "\n",
    "y_test[78] # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18612\\1218574726.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m54\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m180\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m180\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "xi = np.array(X_test[54]).reshape(-1, 180, 180, 1)\n",
    "print(xi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "The Person is Having Dermatofibroma\n"
     ]
    }
   ],
   "source": [
    "prediction = loaded_model_3.predict(xi)\n",
    "# np.argmax(prediction)\n",
    "if np.argmax(prediction) == 0:\n",
    "    print(\"THe Person is Having Acnitic Keratosis\")\n",
    "elif np.argmax(prediction) == 1:\n",
    "    print(\"The Person is Having Dermatofibroma\")\n",
    "else:\n",
    "    print(\"The Person is Having Vascular Lesion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "537a1ccab29e2bad0b4d998c04f52d9d2f6eadd1beb35b9ac347a685a31f8869"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
